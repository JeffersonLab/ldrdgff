{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workflow nets test\n",
    "\n",
    "This notebook is a test to redisign the workflow of gepard using three main components:\n",
    "- Model\n",
    "- Theory\n",
    "- Fitter \n",
    "\n",
    "The idea is to feed a neural network $NN$ with several datasets containing $(xB,t,Q2,val,stat_error,syst_error)$ using just $(xB,t,Q2)$ as inputs. The network is trained to predict the comptom form factors $(ImH, ReH, ImE, ReE, ImHt, ReHt, ImEt, ReEt)$ and the D-term $D(t)$. We will start by predicting two outputs. These outputs are then passed to a theory module which computes the observables values (CS, BSA, etc.).\n",
    "\n",
    "The predicted observables are compared to the input values via loss function $L$. The $NN$ is optimized by minimising the loss.\n",
    "\n",
    "The followin cells provide a walkthrough of how this idea is implemented in code. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First we need to import all the necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import gepard as g\n",
    "import gepard.plots as gplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np \n",
    "#np.set_printoptions(legacy='1.25')\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import shelve, logging, copy\n",
    "logging.basicConfig(level=logging.ERROR) \n",
    "import pandas as pd \n",
    "from scipy.integrate import quad\n",
    "from scipy.interpolate import InterpolatedUnivariateSpline\n",
    "from scipy.stats import norm\n",
    "\n",
    "from math import sqrt\n",
    "from typing import List, Union\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define file path\n",
    "DIR = '/Users/higuera-admin/Documents/Programs/ldrdgff/Analysis/Results'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('/Users/higuera-admin/Documents/Programs/ldrdgff/gepard/src/gepard/')\n",
    "\n",
    "import mydatafiles\n",
    "from mydatafiles import ep2epgamma\n",
    "\n",
    "mydset = g.data.loaddata(mydatafiles)\n",
    "mydset.update(g.data.loaddata(ep2epgamma))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitpoints = mydset[182] + mydset[192] + g.dset[101] + g.dset[102] + g.dset[8]\n",
    "g.describe_data(fitpoints)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory Module\n",
    "\n",
    "Here is the theory module which creates the predicted data based on the neural network predictions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gepard-env-NNP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
